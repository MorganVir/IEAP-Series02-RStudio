---
title: "IEAP-Series02-Rstudio: Correlation and statistical tests\n"
author: "Morgan VIROLAN"
date: "`r format(Sys.time(), '%d %B, %Y, %H:%M')`"
output:
  html_document:
    df_print: paged
    code_folding: hide
    toc: false
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1 Correlations and linear regression
In this series of exercises, we will explore correlations and linear regression, using a dataset from a real experiment.

## 1.1 The problem
We want to know if there is a relationship between the nonuse of the proximal part of the upper limb (PANU) and the nonuse of the shoulder (SANU) or the elbow (EENU).

## 1.2 The data
As we will see, the data is far from perfect, but this is the reality of experimental data, especially in the case of clinical data.

The file "NonUse.csv" contains measures of upper extremity nonuse:

- PANU: Proximal Arm Non Use
- SANU: Shoulder Antepulsion Non Use
- EENU: Elbow Extension Non Use

In the notebook, we'll make a clear description of the experiment, the variables and the data, providing the units of each variable, and the range of possible values.


```{r load-data and create descriptive table}
# loading knit for table generation down the line
library(knitr)
# loading the csv file
nonuse <- read.csv("data/test_data/NonUse.csv" , header = TRUE)
summary(nonuse) 


tbl_PaSaEe <- data.frame(
  Name = c("PANU", "SANU", "EENU"),
  Description = c(
    "Proximal Arm Non Use",
    "Shoulder Antepulsion Non Use",
    "Elbow Extension Non Use"
  ),
  unit = "%",
  range = "0-100"
)

kable(tbl_PaSaEe, caption = "Table 1: Description of the variables in NonUse.csv")
```

## 1.3 Correlation analysis

**Is PANU correlated with SANU and/or EENU?**

We use Spearman’s rank correlation because the data are clinical, not guaranteed to follow a normal distribution, and the relationships may be monotonic but not strictly linear.

```{r correlation tests}
cor.test(nonuse$PANU, nonuse$SANU, method = "spearman")
cor.test(nonuse$PANU, nonuse$EENU, method = "spearman")
```
- PANU and SANU show a weak but significant positive correlation (ρ = 0.22, p = 0.002, both values rounded up).
- PANU and EENU show a moderate and significant positive correlation (ρ = 0.39, p < 0.001, both values rounded up).

This means that higher proximal arm nonuse is associated with higher nonuse of the shoulder and elbow, with a stronger relationship at the elbow level.

**Visualize the PANU-SANU relationship (and the PANU-EENU relationship)**

```{r scatter plots visualisation of the relationships}
library(ggplot2)

ggplot(nonuse, aes(x = PANU, y = SANU)) +
  geom_point(na.rm = TRUE) +
  labs(title = "Scatter plot of PANU vs SANU",
       x = "PANU (%)", y = "SANU (%)") +
  coord_equal() +
  theme_minimal()

ggplot(nonuse, aes(x = PANU, y = EENU)) +
  geom_point(na.rm = TRUE) +
  labs(title = "Scatter plot of PANU vs EENU",
       x = "PANU (%)", y = "EENU (%)") +
  coord_equal() +
  theme_minimal()

```
**Perform a linear regression for PANU-SANU (and for PANU-EENU)**
We do not predict a linear relationship between the variables but we do as we are asked. 

```{r plot-sanu-lm}
library (ggplot2)

ggplot(nonuse, aes(x = SANU, y = PANU)) +
  geom_point(na.rm = TRUE) +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  labs(title = "Scatter plot of PANU vs SANU with regression line",
       x = "SANU (%)",
       y = "PANU (%)") +
  coord_equal() + # auto scale, to have the same scale on both axis
  theme_minimal()

ggplot(nonuse, aes(x = EENU, y = PANU)) +
  geom_point(na.rm = TRUE) +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  labs(title = "Scatter plot of PANU vs EENU with regression line",
       x = "EENU (%)",
       y = "PANU (%)") +
  coord_equal() + # auto scale, to have the same scale on both axis
  theme_minimal()
```


**Give the regression equations: PANU = a * SANU + b and PANU = a * EENU + b**

```{r}
lm_sanu <- lm(PANU ~ SANU, data = nonuse)
summary(lm_sanu)
coef(lm_sanu)
slope <- coef(lm_sanu)[2]
intercept <- coef(lm_sanu)[1]
cat("The regression equation is: PANU =", round(slope,3), "* SANU +", round(intercept,2), "\n")

lm_eenu <- lm(PANU ~ EENU, data = nonuse)
summary(lm_eenu)
coef(lm_eenu)
slope <- coef(lm_eenu)[2]
intercept <- coef(lm_eenu)[1]
cat("The regression equation is: PANU =", round(slope,3), "* EENU +", round(intercept,2), "\n")

```

The linear model PANU ~ SANU yielded the equation:

$$
PANU = 0.101 \times SANU + 8.87
$$

This relationship was not statistically significant* (p = 0.171) and explained less than 1% of the variance (R² = 0.009), indicating very weak predictive power.  

The model PANU ~ EENU yielded the equation:

$$
PANU = 0.399 \times EENU + 5.67
$$

This relationship was statistically significant* (p < 0.001) and explained about 22% of the variance (R² = 0.225), suggesting a modest but meaningful association between proximal arm nonuse and elbow extension nonuse.


**Summarize your results as you would do in your master’s thesis report, or as in a scientific article (e.g., What sentence will you write in the article explaining your result?).**

The model PANU ~ EENU was statistically significant (p < 0.001), with a slope of 0.399. About 22% of the variance in PANU was explained by EENU, indicating a modest but meaningful association. Finally, the analysis is correlational and does not imply causality.

**What are the limits of your analysis?**

These analyses are limited by the large number of missing values (223 observations removed), the weak explanatory power of the PANU ~ SANU model, and the assumptions of linear regression (linearity, normally distributed residuals, homoscedasticity), which may not be fully met. 

# 2 Statistical tests: comparison of medians (or means?)
In this series of exercises, we will explore how to compare two groups of data, using non-parametric tests (or parametric tests… if it makes sense to do so).

## 2.1 Make clear what is a statistical test, and when to use it.
**Below are some questions to guide the analysis.**

For this part, i used an LLM (gpt 5) because some of these concepts are universal, non-controversial, and always defined the same way across sources. 

Prompt used:
“Give me short, clear, for the following: What is a mean? What is a median? [...] Each answer should be concise, one to two sentences.”

- Mean: The mean is the arithmetic average, obtained by summing all observations and dividing by the number of values.
- Median: The median is the middle value of an ordered dataset, with half of the values below and half above.
- Variance: Variance measures how spread out the data are by averaging the squared deviations from the mean.
- Standard deviation: The standard deviation is the square root of the variance, providing a measure of spread in the same units as the data.
 - Statistical test: A statistical test is a procedure used to evaluate a hypothesis, determining whether observed results are compatible with a null hypothesis.
 - When to use a statistical test: A statistical test is used when we want to know whether an observed difference or association is likely due to chance or reflects a real effect.
- Paired vs unpaired tests: A paired test compares two related measurements from the same subjects (e.g., before/after), while an unpaired test compares two independent groups.

he next items required a more thorough investigation. An LLM was used for rephrasing and grammar correction, but not for the content itself.

Normal distribution (or Gaussian) : A symmetric, bell-shaped distribution centered on the mean, where ~68% of values lie within one standard deviation and 95% within two; its importance comes from many sample statistics being normally distributed (Bruce, Bruce & Gedeck, 2020).

Assumption of normality:
This assumption requires that the data (or more precisely, the residuals) follow a normal distribution (see above); in practice, Bruce, Bruce & Gedeck (2020) note that this condition is rarely fully satisfied and is often treated as a last resort.

p-value:
The p-value is the probability of obtaining results at least as extreme as those observed if the assumption of no effect (the null hypothesis) were true. A small p-value means the data would be unlikely under this assumption, which may suggest evidence against it. However, the p-value does not give the probability that the null is true or false, and it does not measure the size or importance of an effect (Amrhein, Greenland & McShane, 2019).

Risk of error in statistical tests:
Every test carries two types of risk. A Type I error (α) occurs when we assert there is an effect when in truth there is none (a false positive). A Type II error (β) occurs when we fail to detect a real effect (a false negative). The probability of a Type II error is inversely related to statistical power: higher power → lower β (StatPearls, 2023).

Assumptions of parametric tests:
Parametric tests assume that the data are drawn from populations following a normal distribution, that observations are independent, and that variances between groups are equal. These assumptions allow estimation of parameters such as means and standard deviations, and if they are violated the validity of the test may be compromised (Nahm, 2016).

Assumptions of non-parametric tests:
Non-parametric tests require fewer assumptions: they mainly assume independence of observations and that data can be ordered or ranked. Because they do not depend on distributional form, they are particularly useful when data are skewed, when variances are unequal, or when sample sizes are too small to meet parametric requirements (Nahm, 2016).

p-value:
A p-value is a number that tells us how much evidence our data give against the idea of no difference between groups. A small p-value can point toward a real difference, but it only makes sense if the study was done well. Bad design, bias, or missing data can make a p-value misleading, so it always needs to be looked at together with confidence intervals and whether the result actually matters in practice (Dorey, 2011).

Bruce, P., Bruce, A., & Gedeck, P. (2020). Practical statistics for data scientists: 50 essential concepts (2nd ed.). O’Reilly Media.

Dorey, F. (2011). Statistics in brief: Interpretation and use of p values: All p values are not equal. Clinical Orthopaedics and Related Research, 469(6), 1819–1821. 

Nahm, F. S. (2016). Nonparametric statistical tests for the continuous data: The basic concept and the practical use. Korean Journal of Anesthesiology, 69(1), 8–14. https://doi.org/10.4097/kjae.2016.69.1.8

Shreffler, J., & Huecker, M. R. (2023). Type I and Type II errors and statistical power. In StatPearls. StatPearls Publishing. https://www.ncbi.nlm.nih.gov/books/NBK557530/


## 2.2 Effect of treatment over time
This is a very classical question in clinical or sport research: does a treatment-training have an effect over time?

To answer this question scientifically, measurements must be taken before and after treatment, and possibly later, in order to assess whether the effect is lasting or not.

## 2.2.1 The data

This file "PrePost.csv" contains before/after measurements. The treatment is a rehabilitation training for individuals with cardiac conditions.

```{r loading-PrePost}
prepost <- read.csv("data/test_data/PrePost.csv", header = TRUE)

# overview of the structure
str(prepost)
summary(prepost)
head(prepost)

```

```{r table-of-the-prepost-variables}
tbl_prepost <- data.frame(
  Variable = c("perf", "time"),
  Description = c("Performance measurement", 
                  "Measurement of time (Before vs After rehab. protocol)"),
  Unit = c("Arbitrary units (workload or HRMax ?)", 
           "Categorical (factor)"),
  `Possible values` = c("Approx. 120-150", "Before / After"),
  `Expected change` = c("Increase after rehabilitation", "-")
)

kable(tbl_prepost, caption = "Table 2: Description of the PrePost.csv variables")

```

Performance measurement could represent either maximum workload (W) or maximum heart rate (HRMax) reached during a functional test. Although the exact population differs (stable coronary artery disease, slightly older patients), studies such as Hambrecht et al. (2004) show that maximum workload and maximum heart rate are commonly reported outcomes with cardiac patients, with values in a similar range to those observed in our dataset. For example, in their trial, patients improved their maximal heart rate from 131 ± 3 bpm to 137 ± 3 bpm and their work capacity from 133±5 to 159±5 (see Table 3 in Hambrecht et al., 2004).


Hambrecht, R., Walther, C., Mobius-Wrinkler, S., Gielen, S., Linke, A., Sabri, O., et al. (2004). Percutaneous coronary angioplasty compared with exercise training in patients with stable coronary artery disease: A randomized trial. Circulation, 109(11), 1374–1380. https://doi.org/10.1161/01.CIR.0000121360.31954.1F


## 2.2.2 The analysis
We now want to know if the treatment has an effect on one or more of the measured variables.

**Does the treatment have an effect?**

We want to compare before vs after rehab on the perf variable. If values increased significantly, it suggests an effect.


**What comparison are we going to make?**

Since it's the same patients that were measured twice (Before / After the rehab), this is a paired comparison:
- Null hypothesis H₀: no difference in perf between before and after.
- Alternative hypothesis H₁: perf after > perf before.

**Which test will you use? Parametric or non-parametric?**

```{r}
library(moments)  # for skewness & kurtosis

# Differences
diffs <- with(prepost, perf[time == "After"] - perf[time == "Before"])

# Shapiro-Wilk test
shapiro.test(diffs)

# Skewness and kurtosis
skew_val <- skewness(diffs)
kurt_val <- kurtosis(diffs)

cat("Skewness:", round(skew_val, 2), "\n")
cat("Kurtosis:", round(kurt_val, 2), "\n")

# Plots
par(mfrow = c(1, 2))
hist(diffs, main = "Histogram of differences", xlab = "After - Before", col = "lightblue", border = "white")
qqnorm(diffs, main = "Q-Q Plot of differences")
qqline(diffs, col = "red")
par(mfrow = c(1, 1))

```

We checked normality with Shapiro–Wilk (W = 0.90, p = 0.283). Skewness (0.61), kurtosis (2.75), and the plots also looked fine. Following Razali & Wah (2011), we added these checks, but with only n = 8 normality tests have little power (Razali & Wah, 2011; Ghasemi & Zahediasl, 2012). So we went with the Wilcoxon signed-rank test, which fits our hypothesis (paired differences) and is safer with small samples (McDonald, 2014). 

Since we don’t really know what `perf` measures, we can’t be sure if higher or lower values mean improvement. To stay on the safe side, we used a two-sided Wilcoxon test to check for any effect of rehab in either direction, as recommended by UCLA’s FAQ on one- vs two-tailed tests (UCLA OARC, 2024).


- Ghasemi, A., & Zahediasl, S. (2012). Normality tests for statistical analysis: A guide for non-parametric methods. International Journal of Endocrinology and Metabolism, 10(2), 486–489. https://doi.org/10.5812/ijem.3505

- McDonald, J. H. (2014). Handbook of Biological Statistics (3rd ed., p. 32). Sparky House Publishing. http://www.biostathandbook.com

- Razali, N. M., & Wah, Y. B. (2011). Power comparisons of Shapiro–Wilk, Kolmogorov–Smirnov, Lilliefors and Anderson–Darling tests. Journal of Statistical Modeling and Analytics, 2(1), 21–33.

- UCLA: Statistical Consulting Group. (2024). FAQ: What are the differences between one-tailed and two-tailed tests? UCLA Institute for Digital Research and Education. Retrieved from https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-the-differences-between-one-tailed-and-two-tailed-tests/

```{r wilcoxong signed test}
wilcox.test(
  prepost$perf[prepost$time == "Before"],
  prepost$perf[prepost$time == "After"],
  paired = TRUE,
  alternative = "two.sided"
)

# Medians for reporting
tapply(prepost$perf, prepost$time, median)

```

The test showed a significant difference between Before and After (V = 0, p ≈ 0.014).
Median performance increased from 135.5 (Before) to 137.5 (After).

R gives a warning about ties (“exact p-value not computable with ties”), which occurred because several participants showed identical change scores (e.g., +1, +2, +3). This leads to ties in the ranking of differences. In such cases, R automatically switches to an approximate p-value with continuity correction.


**Which graph illustrates the effect of the treatment?**

```{r spaghetti graph}
library(ggplot2)
library(dplyr)
library(tidyr)

# Reconstruct subject IDs because it's missing on the prepost.csv
prepost <- prepost %>%
  group_by(time) %>%
  mutate(row = row_number()) %>%
  tidyr::pivot_wider(names_from = time, values_from = perf) %>%
  mutate(id = row_number()) %>%
  tidyr::pivot_longer(cols = c("Before", "After"),
                      names_to = "time", values_to = "perf")

# Ensure proper order
prepost <- prepost %>%
  mutate(time = factor(time, levels = c("Before", "After")))

# Plot: 2 boxplots + spaghetti lines
ggplot(prepost, aes(x = time, y = perf)) +
  geom_boxplot(outlier.shape = NA, fill = "lightgray") +
  geom_line(aes(group = id), colour = "gray60", alpha = 0.7) +
  geom_point(size = 2) +
  labs(x = NULL,
       y = "Performance (AU)",
       title = "Performance Before and After Rehabilitation") +
  theme_minimal(base_size = 14)

```

**What sentence will you write in your thesis to explain your result?**

Participants showed a small but consistent improvement after rehabilitation, with all trajectories going upward and no ill or adverse effects. The Wilcoxon signed-rank test confirmed the change was significant (V = 0, p ≈ 0.014), with median values rising from 135.5 to 137.5 (Arbitrary Units as we could say for sure what was the actual performance metric). Assuming that higher perf reflects better functional capacity, this points to a positive effect of the program.


## 2.3 Testing some stereotypes
Humans have stereotypes, some of them are probably true, some others are probably false (e.g., https://pmc.ncbi.nlm.nih.gov/articles/PMC9850150/).

Here, we will test stereotypes about snorers… among others.

## 2.3.1 The data
This file contains anthropometric and qualitative measurements (1 person per line).

```{r data-load snore.txt}
snore <- read.table("data/test_data/snore.txt", header = TRUE)

# Quick structure and overview
str(snore)
summary(snore)
head(snore)

```

```{r}
tbl_snore <- data.frame(
  Variable = c("age", "weight", "height", "alcool", "sex", "snore", "tabac"),
  Description = c(
    "Age of participant",
    "Body weight",
    "Body height",
    "Alcohol consumption",
    "Sex (H = male, F = female)",
    "Snoring status (O = yes, N = no)",
    "Smoking status (O = yes, N = no)"
  ),
  Unit = c("Years", "kg", "cm", "Frequency score", "-", "-", "-"),
  `Possible values` = c("23-74", "42-120", "158-208", "0-15", "H/F", "O/N", "O/N")
)

knitr::kable(tbl_snore, caption = "Table 3: Description of the variables in snore.txt")

```


## 2.3.2 The analysis
**Do the data confirm the following stereotypes? Provide a reasoned answer (data, figure, result sentence) for each question.**

```{r snore-analysis}
library(ggplot2)
library(dplyr)

# 1. Are snorers fatter? (weight ~ snore)
ggplot(snore, aes(x = snore, y = weight)) +
  geom_jitter(width = 0.2, alpha = 0.7, size = 2) +
  stat_summary(fun = median, geom = "crossbar", width = 0.5,
               colour = "red", fatten = 2) +
  labs(x = "Snoring status (O = yes, N = no)", y = "Weight (kg)",
       title = "Weight by snoring status") +
  theme_minimal(base_size = 14)

wilcox.test(weight ~ snore, data = snore)

# 2. Do snorers drink OR smoke more? (combined variable). Wasn't sure about this one, chose
# to interpret it as "do snorers have higher prevalence of drinking or smoking?" but i could be wrong.

snore <- snore %>%
  mutate(drink_or_smoke = ifelse(alcool > 0 | tabac == "O", "Yes", "No"))

drinksmoke_table <- table(snore$snore, snore$drink_or_smoke)
drinksmoke_table
chisq.test(drinksmoke_table)


# 3. Are men fatter? (weight ~ sex)
ggplot(snore, aes(x = sex, y = weight)) +
  geom_jitter(width = 0.2, alpha = 0.7, size = 2) +
  stat_summary(fun = median, geom = "crossbar", width = 0.5,
               colour = "red", fatten = 2) +
  labs(x = "Sex (H = male, F = female)", y = "Weight (kg)",
       title = "Weight by sex") +
  theme_minimal(base_size = 14)

wilcox.test(weight ~ sex, data = snore)


# 4. Do women smoke less? (tabac × sex)
tabac_sex <- table(snore$sex, snore$tabac)
tabac_sex
chisq.test(tabac_sex)


# 5. Correlations among continuous variables
num_vars <- snore[, c("age", "weight", "height", "alcool")]
cor(num_vars, use = "pairwise.complete.obs", method = "spearman")

```

Normality could not be assumed in the subgroups, particularly for skewed variables like alcohol consumption, so we used non-parametric tests (Mann–Whitney, chi-square, Spearman).

For comparisons of continuous variables between two independent groups (e.g., snorers vs non-snorers, men vs women), we used the Wilcoxon rank-sum test (Mann–Whitney U), the non-parametric analogue of the two-sample t-test (McDonald, 2014).

Categorical associations (e.g., snoring × smoking, or snoring × combined drink/smoke behavior) were examined with chi-square tests of independence (McDonald, 2014).

Correlations between continuous variables were assessed with Spearman’s rank correlation, a non-parametric alternative to Pearson’s correlation (McDonald, 2014).

Results :

- Snorers were not heavier than non-snorers (Wilcoxon rank-sum test, W = 1154, p = 0.91).
- Snorers were not more likely to drink or smoke overall than non-snorers (χ²(1) < 0.01, p = 1).
- Men did not differ significantly from women in weight (Wilcoxon rank-sum test, W = 893.5, p = 0.73).
- Women were significantly less likely to smoke than men (χ²(1) = 7.00, p = 0.008).

**From a more general perspective: Are there any correlations between variables?** 

As expected, weight and height were very strongly correlated (ρ ≈ 0.93). Other associations were weak, suggesting that in this dataset age and alcohol consumption were not closely related to body size.

3 Epilogue: beyond the scope of this series
Finally, some of you may want to go beyond the scope of this series with new questions, such as:

Can you predict the value of one variable from the others? (linear regression, logistic regression, etc.)
Can you classify individuals based on their variables? (clustering, PCA, etc.)
NOTE: These questions are more advanced, and I don’t want you to address them from a technical standpoint or provide code. But they give you an idea of what you can do with data mining techniques… provided you know why you want to do it. I invite you to think about the perspective these questions open up and, if you feel ready to do so, to write a short paragraph about it.
